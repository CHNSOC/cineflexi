{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Background"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MYdW_qVmfNX_"},"source":["\n","\n","**MovieLens** is a movie recommendation system operated by GroupLens, a research group at the University of Minnesota. \n","\n","1. Propose and implement your own recommendation system based on the MovieLens dataset. Use `ratings_train.csv` as the training set, `ratings_valid.csv` as the validation set. Your system may use information from `movies.csv` and `tags.csv` to conduct recommendations. The undisclosed test set will be used to evaluate your system.\n","   - The data file structure is available at https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html. \n","   - The main goal of the recommendation system is to minimize the root-mean-square error.\n","   - The implementation should include a function named `predict_rating`. This function accepts a DataFrame with two columns `userId` and `movieId`. Then, the function adds a column named `rating` storing a predicted rating of a `movieId` by a `userId`.\n","   - Your program must return a root-mean-square error value when the validation set is changed to another file. Otherwise, your score will be deducted by 50%.\n","   - You must modify the given program to make better recommendations. Submitting the original program without modification is considered plagiarism.\n","2. Prepare slides for a 7-minute presentation to explain your proposed technique and algorithm to conduct recommendation, and show your RMSE results on the validation set.\n","3. Submit all required documents by April 30, 2023; 23:59. Late submission will not be accepted and will be marked 0. Do not wait until the last minute. Plagiarism and code duplication will be checked. \n","4. Present your work on May 1, 2023 within 7 minutes. Exceeding 7 minutes will be subject to point deduction."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PTo0T98IfNYB"},"source":["### Loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682326803856,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"fbz2Ggf8fNYA"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"elapsed":10,"status":"error","timestamp":1682326803857,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"YUW0xHegfNYB","outputId":"9b739d17-464a-49a1-8b2a-926158225229"},"outputs":[],"source":["ratings_train = pd.read_csv('datasets/ratings_train.csv')\n","ratings_valid = pd.read_csv('datasets/ratings_valid.csv')\n","movies = pd.read_csv('datasets/movies.csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Recommendation system based on User/Rating Matrix and Latent Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","torch.manual_seed(1337)\n","\n","class MatrixFactorization(torch.nn.Module):\n","    def __init__(self, n_users, n_items, n_factors=20):\n","        super().__init__()\n","        # Embeddings -> A simple lookup table that stores embeddings of a fixed dictionary and size.\n","        \n","        self.user_factors = torch.nn.Embedding(n_users, n_factors) # user\n","        self.item_factors = torch.nn.Embedding(n_items, n_factors) # item\n","        self.user_factors.weight.data.uniform_(0, 0.05)\n","        self.item_factors.weight.data.uniform_(0, 0.05)\n","        \n","    def forward(self, data):\n","        users, items = data[:,0], data[:,1]\n","        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Data Preprocessing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Helper functions to switch b/w IDs to Indexes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_users = ratings_train.userId.unique()\n","all_movies = ratings_train.movieId.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["usrid_to_idx = {o: i for i, o in enumerate(all_users)}\n","movid_to_idx = {o: i for i, o in enumerate(all_movies)}\n","\n","idx_to_movid = {i: o for o, i in usrid_to_idx.items()}\n","idx_to_usrid = {i: o for o, i in movid_to_idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# ratings_train_orig = ratings_train.copy()\n","\n","ratings_train.userId = ratings_train.userId.apply(lambda x: usrid_to_idx[x])\n","ratings_train.movieId = ratings_train.movieId.apply(lambda x: movid_to_idx[x])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_batch():\n","    x = ratings_train.drop(['rating', 'timestamp'], axis=1).values\n","    y = ratings_train['rating'].values\n","    x, y = torch.tensor(x), torch.tensor(y)\n","    return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 1024\n","\n","model = MatrixFactorization(len(all_users), len(all_movies), n_factors=8)\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","# MSE loss\n","loss_fn = torch.nn.MSELoss()\n","\n","# ADAM optimizier\n","optimizer = torch.optim.Adam(model.parameters(), lr=4e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for it in range(num_epochs + 1):\n","    losses = []\n","    x, y = get_batch()\n","    x, y = x.cuda(), y.cuda()\n","    optimizer.zero_grad()\n","    outputs = model(x)\n","    loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n","    losses.append(loss.item())\n","    loss.backward()\n","    optimizer.step()\n","    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))\n","     "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Transform embeddings to numpy matrix for prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()\n","trained_user_embeddings = model.user_factors.weight.data.cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(user, item):\n","    return np.dot(trained_user_embeddings[user,:],trained_movie_embeddings[item,:]) # np.dot(self.P[u,:],self.Q[i,:])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prediction\n","def predict_rating(df):\n","    # Input: \n","\t# \tdf = a dataframe with two columns: userId, movieId\n","\tdf_pred = ratings_valid[['userId', 'movieId']].copy()\n","\tdf_pred.loc[:, 'pred_Rating'] = df.apply(lambda row: predict(usrid_to_idx[row['userId']], movid_to_idx[row['movieId']]), axis=1)\n","\t# Output:\n","\t#   a dataframe with three columns: userId, movieId, rating\n","\treturn df_pred\n","\n","# Prepare df for prediction\n","r = ratings_valid[['userId', 'movieId']]\n","\n","# Predict ratings\n","ratings_pred = predict_rating(r)\n","\n","from sklearn.metrics import mean_squared_error\n","\n","r_true = ratings_valid['rating'].to_numpy()\n","r_pred = ratings_pred['pred_Rating'].to_numpy()\n","\n","rmse = mean_squared_error(r_true, r_pred, squared=False)\n","print(f\"RMSE = {rmse:.4f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Recommendation system based on Transformer Architecture - Prototype"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["It doesn't work at current state.\n","The data is all messed up, loss > 1 mil so glhf\n","\n","Refs:\n","\n","https://github.com/pytorch/examples/tree/main/word_language_model -> Pytorch Transformer Example\n","\n","https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py -> Build GPT Step by step (Implementing Transformer Architecture)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the dataset using pandas\n","import pandas as pd\n","\n","train_data_tensor = pd.read_csv(\"datasets/ratings_train.csv\")\n","test_data = pd.read_csv(\"datasets/ratings_valid.csv\")\n","\n","# Preprocess the data\n","# ...\n","from sklearn.model_selection import train_test_split\n","\n","train_data_tensor, test_data = train_test_split(train_data_tensor, test_size=0.2)\n","\n","# Build the Transformer-based model using PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define the hyperparameters and optimizer for the model\n","input_dim = 24\n","block_size = 3\n","batch_size = 64\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data_tensor.drop(['timestamp'], axis=1, inplace=True)\n","test_data.drop(['timestamp'], axis=1, inplace=True)\n","train_data_tensor = torch.tensor(train_data_tensor.values, dtype=torch.float)\n","test_data_tensor = torch.tensor(test_data.values, dtype=torch.float)\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data_tensor if split == 'train' else test_data_tensor\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","transformer_model = nn.Transformer(d_model=3, nhead=3, num_encoder_layers=12)\n","transformer_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","import time\n","\n","\n","ntokens = 9\n","sequence_length = 9\n","criterion = nn.MSELoss()\n","epochs = 40\n","lr = 20 \n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    transformer_model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","\n","    for batch, _ in enumerate(range(0, train_data_tensor.size(0) - 1, sequence_length)):\n","        data, targets = get_batch('train')\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        transformer_model.zero_grad()\n","        output = transformer_model(data, targets)\n","        output = output.view(-1, ntokens)\n","        targets = targets.view(64, 9)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.25) # 0.25 -> gradient clip\n","        for p in transformer_model.parameters():\n","            p.data.add_(p.grad, alpha=-lr)\n","\n","        total_loss += loss.item()\n","\n","        if batch % 100 == 0 and batch > 0:\n","            cur_loss = total_loss / 100\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epochs, batch, len(train_data_tensor) // sequence_length, lr,\n","                elapsed * 1000 / 100, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    transformer_model.eval()\n","    total_loss = 0.\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, sequence_length):\n","            data, targets = get_batch(data_source, i)\n","            output = transformer_model(data)\n","            output = output.view(-1, ntokens)\n","            targets = targets.view(64, 9)\n","            total_loss += len(data) * criterion(output, targets.view(64, 9)).item()\n","    return total_loss / (len(data_source) - 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_val_loss = None\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(test_data_tensor)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
