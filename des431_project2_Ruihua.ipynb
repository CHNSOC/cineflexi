{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Background"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MYdW_qVmfNX_"},"source":["\n","\n","**MovieLens** is a movie recommendation system operated by GroupLens, a research group at the University of Minnesota. \n","\n","1. Propose and implement your own recommendation system based on the MovieLens dataset. Use `ratings_train.csv` as the training set, `ratings_valid.csv` as the validation set. Your system may use information from `movies.csv` and `tags.csv` to conduct recommendations. The undisclosed test set will be used to evaluate your system.\n","   - The data file structure is available at https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html. \n","   - The main goal of the recommendation system is to minimize the root-mean-square error.\n","   - The implementation should include a function named `predict_rating`. This function accepts a DataFrame with two columns `userId` and `movieId`. Then, the function adds a column named `rating` storing a predicted rating of a `movieId` by a `userId`.\n","   - Your program must return a root-mean-square error value when the validation set is changed to another file. Otherwise, your score will be deducted by 50%.\n","   - You must modify the given program to make better recommendations. Submitting the original program without modification is considered plagiarism.\n","2. Prepare slides for a 7-minute presentation to explain your proposed technique and algorithm to conduct recommendation, and show your RMSE results on the validation set.\n","3. Submit all required documents by April 30, 2023; 23:59. Late submission will not be accepted and will be marked 0. Do not wait until the last minute. Plagiarism and code duplication will be checked. \n","4. Present your work on May 1, 2023 within 7 minutes. Exceeding 7 minutes will be subject to point deduction."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PTo0T98IfNYB"},"source":["### Loading data"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682326803856,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"fbz2Ggf8fNYA"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"elapsed":10,"status":"error","timestamp":1682326803857,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"YUW0xHegfNYB","outputId":"9b739d17-464a-49a1-8b2a-926158225229"},"outputs":[],"source":["ratings_train = pd.read_csv('datasets/ratings_train.csv')\n","ratings_valid = pd.read_csv('datasets/ratings_valid.csv')\n","movies = pd.read_csv('datasets/movies.csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Nn1Y4glTfNYB"},"source":["### Constructing model and predicting ratings"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1682326803857,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"6Cajj0vyfNYB"},"outputs":[],"source":["# Model construction\n","avg_rating = ratings_train[['movieId', 'rating']].groupby(by='movieId').mean()\n","\t    \n","# Prediction\n","def predict_rating(df):\n","    # Input: \n","\t# \tdf = a dataframe with two columns: userId, movieId\n","\t# Output:\n","\t#   a dataframe with three columns: userId, movieId, rating\n","\treturn df.join(avg_rating, on='movieId')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1682326803858,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"-SKuG3x7fNYB"},"outputs":[],"source":["# Prepare df for prediction\n","r = ratings_valid[['userId', 'movieId']]\n","\n","# Predict ratings\n","ratings_pred = predict_rating(r)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1682326803858,"user":{"displayName":"Cholwich Nattee","userId":"12173633630694553387"},"user_tz":-420},"id":"uMQRnNDVfNYB"},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE = 0.9171\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","\n","r_true = ratings_valid['rating'].to_numpy()\n","r_pred = ratings_pred['rating'].to_numpy()\n","\n","rmse = mean_squared_error(r_true, r_pred, squared=False)\n","print(f\"RMSE = {rmse:.4f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Recommendation systen based on Transformer Architecture - Prototype"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["It doesn't work rn, in case you can't see XDDD"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Load the dataset using pandas\n","import pandas as pd\n","\n","train_data_tensor = pd.read_csv(\"datasets/ratings_train.csv\")\n","test_data = pd.read_csv(\"datasets/ratings_valid.csv\")\n","\n","# Preprocess the data\n","# ...\n","from sklearn.model_selection import train_test_split\n","\n","train_data_tensor, test_data = train_test_split(train_data_tensor, test_size=0.2)\n","\n","# Build the Transformer-based model using PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define the hyperparameters and optimizer for the model\n","input_dim = 24\n","block_size = 3\n","batch_size = 64\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["train_data_tensor.drop(['timestamp'], axis=1, inplace=True)\n","test_data.drop(['timestamp'], axis=1, inplace=True)\n","train_data_tensor = torch.tensor(train_data_tensor.values, dtype=torch.float)\n","test_data_tensor = torch.tensor(test_data.values, dtype=torch.float)\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data_tensor if split == 'train' else test_data_tensor\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["Transformer(\n","  (encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-11): 12 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n","        )\n","        (linear1): Linear(in_features=3, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=3, bias=True)\n","        (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (layers): ModuleList(\n","      (0-5): 6 x TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n","        )\n","        (linear1): Linear(in_features=3, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=3, bias=True)\n","        (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\n","transformer_model = nn.Transformer(d_model=3, nhead=3, num_encoder_layers=12)\n","transformer_model.to(device)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["import math\n","import time\n","\n","\n","ntokens = 9\n","sequence_length = 9\n","criterion = nn.MSELoss()\n","epochs = 40\n","lr = 20 \n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    transformer_model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","\n","    for batch, _ in enumerate(range(0, train_data_tensor.size(0) - 1, sequence_length)):\n","        data, targets = get_batch('train')\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        transformer_model.zero_grad()\n","        output = transformer_model(data, targets)\n","        output = output.view(-1, ntokens)\n","        targets = targets.view(64, 9)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.25) # 0.25 -> gradient clip\n","        for p in transformer_model.parameters():\n","            p.data.add_(p.grad, alpha=-lr)\n","\n","        total_loss += loss.item()\n","\n","        if batch % 100 == 0 and batch > 0:\n","            cur_loss = total_loss / 100\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epochs, batch, len(train_data_tensor) // sequence_length, lr,\n","                elapsed * 1000 / 100, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    transformer_model.eval()\n","    total_loss = 0.\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, sequence_length):\n","            data, targets = get_batch(data_source, i)\n","            output = transformer_model(data)\n","            output = output.view(-1, ntokens)\n","            targets = targets.view(64, 9)\n","            total_loss += len(data) * criterion(output, targets.view(64, 9)).item()\n","    return total_loss / (len(data_source) - 1)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n","torch.Size([64, 9]) torch.Size([64, 9])\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"_pydevd_bundle/pydevd_cython.pyx\", line 577, in _pydevd_bundle.pydevd_cython.PyDBFrame._handle_exception\n","  File \"_pydevd_bundle/pydevd_cython.pyx\", line 312, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n","  File \"c:\\Users\\Reychard\\Cineflexi\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2070, in do_wait_suspend\n","    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n","  File \"c:\\Users\\Reychard\\Cineflexi\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2106, in _do_wait_suspend\n","    time.sleep(0.01)\n","KeyboardInterrupt\n"]}],"source":["best_val_loss = None\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(test_data_tensor)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
